---
title: "Neural_Network"
output: html_document
date: "2023-10-07"
---

```{r setup, include=FALSE}
library(tidyverse)
library(keras)
```

```{r}
data <- read_csv("for_nt.csv") %>% select(-name)
head(data)
write_csv(data,"data.csv")


```

```{r}
library(keras)
library(caret)


# Convert string variables to factors
data$language <- as.factor(data$language)
data$device <- as.factor(data$device)
data$error <- as.factor(data$error)
data$trend <- as.factor(data$trend)
data$seasonality <- as.factor(data$seasonality)

# Encode categorical variables
dmy <- dummyVars(" ~ .", data = data[,1:5])
encoded_data <- data.frame(predict(dmy, newdata = data[,1:5]))

# Extract and normalize time series data
time_series_data <- data[, 6:ncol(data)]
normalized_time_series <- scale(time_series_data)

# Combine encoded and normalized data
final_data <- cbind(encoded_data, normalized_time_series)

# Split the data into training and testing sets
set.seed(123) 
sample_index <- sample(seq_len(nrow(final_data)), size = floor(0.8 * nrow(final_data)))

x_train <- as.matrix(final_data[sample_index, -ncol(final_data)])
y_train <- as.matrix(final_data[sample_index, ncol(final_data)])

x_test <- as.matrix(final_data[-sample_index, -ncol(final_data)])
y_test <- as.matrix(final_data[-sample_index, ncol(final_data)])

# Define the model
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.001),
  loss = 'mse',
  metrics = c('mae')
)


# Train the model
history <- model %>% fit(
  x_train, y_train,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2
)

# Evaluate the model
model %>% evaluate(x_test, y_test)

# Make predictions
predictions <- model %>% predict(x_test)

```

```{r}
library(ggplot2)

# Extract training history
history_data <- as.data.frame(history$metrics)

# Generate epoch vector
epochs <- seq(1, nrow(history_data))

# Plotting Loss
p1 <- ggplot(history_data, aes(x = epochs)) +
  geom_line(aes(y = loss, col = "Training")) +
  geom_line(aes(y = val_loss, col = "Validation")) +
  labs(title = "Model Loss", x = "Epochs", y = "Loss") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

# Plotting Mean Absolute Error (if available)
p2 <- NULL
if ("mae" %in% names(history_data)) {
  p2 <- ggplot(history_data, aes(x = epochs)) +
    geom_line(aes(y = mae, col = "Training")) +
    geom_line(aes(y = val_mae, col = "Validation")) +
    labs(title = "Mean Absolute Error", x = "Epochs", y = "MAE") +
    scale_color_manual(values = c("blue", "red")) +
    theme_minimal()
}

# Display plots
p1
if (!is.null(p2)) p2

```


``{r}
library(keras)
library(tidyr)
library(dplyr)



# Extract page names
page_names <- unique(data$name)

# Initialize a dataframe to store accuracy measures
accuracy_df <- data.frame(page=character(), train_mae=numeric(), test_mae=numeric(), stringsAsFactors=FALSE)

# Directory to save models
save_dir <- "~/my_saved_models/"

# Loop through each page, train a model, and save it
for (page in page_names) {
  
  # Extract data for the current page
  page_data <- data[data$name == page, 7:ncol(data)]
  
  # Convert data frame to a vector (assuming we're working with 1D time series)
  page_data <- as.vector(t(page_data))
  
  # Transform data: Create sequences of 7 days as input (X) and the 8th day as output (Y)
  n <- length(page_data)
  X <- sapply(1:(n-7), function(i) page_data[i:(i+6)])
  Y <- page_data[8:n]
  
  # Train-test split (e.g., use the first 80% of data points for training)
  train_size <- floor(0.8 * ncol(X))
  X_train <- X[, 1:train_size]
  Y_train <- Y[1:train_size]
  X_test <- X[, (train_size + 1):ncol(X)]
  Y_test <- Y[(train_size + 1):length(Y)]
  
  # Define model architecture
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = 'relu', input_shape = 7) %>%
    layer_dense(units = 64, activation = 'relu') %>%
    layer_dense(units = 1)
  
  # Compile model
  model %>% compile(
    optimizer = optimizer_adam(learning_rate = 0.001),
    loss = "mean_squared_error",
    metric = "mean_absolute_error"
  )
  
  # Train model
  history <- model %>% fit(
    x = t(X_train), y = Y_train, 
    epochs = 50, batch_size = 32, 
    validation_split = 0.2
  )
  
  # Predict and evaluate
  preds_train <- model %>% predict(t(X_train))
  preds_test <- model %>% predict(t(X_test))
  
  train_mae <- mean(abs(preds_train - Y_train))
  test_mae <- mean(abs(preds_test - Y_test))
  
  # Ensure the directory exists
  dir.create(save_dir, showWarnings = FALSE)
  
  # Save model
  save_model_hdf5(model, paste0(save_dir, page, "_model.keras"))
  
  # Store accuracy measures
  accuracy_df <- rbind(accuracy_df, data.frame(page=page, train_mae=train_mae, test_mae=test_mae))
  
  # Clean up resources
  k_clear_session()
}

# Display accuracy measures
print(accuracy_df)

```



