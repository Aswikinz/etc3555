---
title: "Neural_Network"
output: html_document
date: "2023-10-07"
---

```{r setup, include=FALSE}
library(tidyverse)
library(keras)
```

```{r}
data <- read_csv("for_nt.csv") %>% select(-name)
head(data)
write_csv(data,"data.csv")


```

```{r}
library(keras)
library(caret)


# Convert string variables to factors
data$language <- as.factor(data$language)
data$device <- as.factor(data$device)
data$error <- as.factor(data$error)
data$trend <- as.factor(data$trend)
data$seasonality <- as.factor(data$seasonality)

# Encode categorical variables
dmy <- dummyVars(" ~ .", data = data[,1:5])
encoded_data <- data.frame(predict(dmy, newdata = data[,1:5]))

# Extract and normalize time series data
time_series_data <- data[, 6:ncol(data)]
normalized_time_series <- scale(time_series_data)

# Combine encoded and normalized data
final_data <- cbind(encoded_data, normalized_time_series)

# Split the data into training and testing sets
set.seed(123) 
sample_index <- sample(seq_len(nrow(final_data)), size = floor(0.8 * nrow(final_data)))

x_train <- as.matrix(final_data[sample_index, -ncol(final_data)])
y_train <- as.matrix(final_data[sample_index, ncol(final_data)])

x_test <- as.matrix(final_data[-sample_index, -ncol(final_data)])
y_test <- as.matrix(final_data[-sample_index, ncol(final_data)])

# Define the model
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.001),
  loss = 'mse',
  metrics = c('accuracy')
)


# Train the model
history <- model %>% fit(
  x_train, y_train,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2
)

# Evaluate the model
model %>% evaluate(x_test, y_test)

# Make predictions
predictions <- model %>% predict(x_test)

```

```{r}
library(ggplot2)

# Extract training history
history_data <- as.data.frame(history$metrics)

# Generate epoch vector
epochs <- seq(1, nrow(history_data))

# Plotting Loss
p1 <- ggplot(history_data, aes(x = epochs)) +
  geom_line(aes(y = loss, col = "Training")) +
  geom_line(aes(y = val_loss, col = "Validation")) +
  labs(title = "Model Loss", x = "Epochs", y = "Loss") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

# Plotting Mean Absolute Error (if available)
p2 <- NULL
if ("mae" %in% names(history_data)) {
  p2 <- ggplot(history_data, aes(x = epochs)) +
    geom_line(aes(y = mae, col = "Training")) +
    geom_line(aes(y = val_mae, col = "Validation")) +
    labs(title = "Mean Absolute Error", x = "Epochs", y = "MAE") +
    scale_color_manual(values = c("blue", "red")) +
    theme_minimal()
}

# Display plots
p1
if (!is.null(p2)) p2

```
```{r}
# Assuming `predictions` are your model's predictions and `y_test` are the actual values

# Ensure predictions are the same length as y_test
if(length(predictions) != length(y_test)) {
  stop("Predictions and actual values have different lengths!")
}

# Compute Metrics
mae <- mean(abs(predictions - y_test))  # Mean Absolute Error
mse <- mean((predictions - y_test)^2)    # Mean Squared Error
rmse <- sqrt(mse)                        # Root Mean Squared Error

# Print Metrics
cat("Mean Absolute Error: ", mae, "\n")
cat("Mean Squared Error: ", mse, "\n")
cat("Root Mean Squared Error: ", rmse, "\n")

```

