---
title: "Neural_Network"
output: html_document
date: "2023-10-07"
---

```{r setup, include=FALSE}
library(tidyverse)
library(keras)
library(caret)
```




Dont run this chunk 

```{r ets using df_long , eval = FALSE}
#u dont have to run this chunk, its already been processed and saved, skip to the next 
plan(multisession)
# Ensure 'models/' and 'forecasts/' directories exist
dir.create("models", showWarnings = FALSE)
dir.create("forecasts", showWarnings = FALSE)
df_long_filter <- df_long %>%
  select(Page, date, clicks) %>%
  mutate(date = as.Date(date, format="%Y-%m-%d"))
# Convert the data into a tsibble object (time series tibble)
data_tsibble <- df_long_filter %>% 
  as_tsibble(index = date, key = Page)
pages <- unique(df_long$Page)
page_filenames <- make.names(pages, unique = TRUE)
# Fit models in parallel and save them to RDS files
future_map2(pages, page_filenames, function(page, filename) {
  # Filter data for the specific page
  page_data <- data_tsibble %>% filter(Page == page)
  
  # Fit the ETS model
  model <- page_data %>%
    model(ETS(clicks))
  
  # Save the model to an RDS file, named by page
  saveRDS(model, paste0("models/", filename, ".rds"))
})
# Generate forecasts in parallel and save them
future_map(page_filenames, function(filename) {
  # Load the model from the RDS file
  model <- readRDS(paste0("models/", filename, ".rds"))
  
  # Generate forecasts
  forecasts <- model %>% forecast(h = "30 days")
  
  # Save the forecasts to an RDS file
  saveRDS(forecasts, paste0("forecasts/", filename, ".rds"))
})
# When needed, load forecasts and combine them for plotting/analysis
forecasts_list <- future_map(page_filenames, ~readRDS(paste0("forecasts/", .x, ".rds")))
forecasts_combined <- bind_rows(forecasts_list)
```

dont run this 

```{r load the dataframe back from the files}

model_files <- list.files(path = "models/", pattern = "*.rds", full.names = TRUE)
# Read and bind data
model_list <- lapply(model_files, readRDS)
model_ets_df <- bind_rows(model_list)
```


run from here on
```{r}
# Load the datasets
ets_model <- read_csv("ets_model.csv")
country_device_df <- read_csv("country_device_df.csv")

# Cleaning: Remove duplicate names from country_device_df
country_device_df_clean <- country_device_df %>%
    distinct(name, .keep_all = TRUE)

# Extract error, trend, and seasonality from ets(clicks) in ets_model
# Ensure the column name containing "<ETS(...)>" is accurate.
ets_model_clean <- ets_model %>%
    mutate(
        ETS = str_extract(`ETS(clicks)`, "(?<=<ETS\\().*?(?=\\)>)"),
        error = sapply(strsplit(ETS, ","), "[[", 1),
        trend = sapply(strsplit(ETS, ","), "[[", 2),
        seasonality = sapply(strsplit(ETS, ","), "[[", 3)
    ) %>%
    select(-ETS, -`ETS(clicks)`) # Omit unnecessary columns

# Joining: Merge country_device_df_clean with ets_model_clean
# Ensure that the joining column "name" is correctly identified and prepared in both dataframes.
merged_df <- merge(country_device_df_clean, ets_model_clean, by = "Page", all.x = TRUE)

merged_df <- merged_df %>%
    select(name, language, device, error, trend, seasonality,everything()) %>% select(-Page)

merged_df <- merged_df %>%
    mutate(
        error = ifelse(is.na(error), "notavailable", error),
        trend = ifelse(is.na(trend), "notavailable", trend),
        seasonality = ifelse(is.na(seasonality), "notavailable", seasonality)
    )

```

```{r}
data <- merged_df %>% select(-name,-ncol(merged_df))
head(data)

```
```{r}
data$language <- as.factor(data$language)
data$device <- as.factor(data$device)
data$error <- as.factor(data$error)
data$trend <- as.factor(data$trend)
data$seasonality <- as.factor(data$seasonality)

# Encode categorical variables
dmy <- dummyVars(" ~ .", data = data[,1:5])
encoded_data <- data.frame(predict(dmy, newdata = data[,1:5]))

# Extract and normalize time series data
time_series_data <- data[, 6:ncol(data)]
# normalized_time_series <- scale(time_series_data)

# Combine encoded and data
final_data <- cbind(encoded_data, time_series_data)
head(final_data)

index_page <- 1:nrow(final_data) # To define which prediction is for which page

final_data_long <- final_data %>%
  mutate(index = index_page) %>%
  pivot_longer(cols = starts_with("2017")|starts_with("2018")|starts_with("2019")|starts_with("2020"),
               names_to = "date", values_to = "page_visits") %>%
  mutate(date = as.Date(date))

head(final_data_long)
dim(final_data_long)

```

```{r}
# Split train and test
cutoff_date <- as.Date("2019-12-17")

train_data <- subset(final_data_long,date <= cutoff_date)
test_data <- subset(final_data_long,date > cutoff_date)

head(final_data_long)
head(train_data)
tail(train_data)

# Normalise the page_visit columns
train_mean <- mean(train_data$page_visits)
train_sd <- sd(train_data$page_visits)
test_mean <- mean(test_data$page_visits)
test_sd <- sd(test_data$page_visits)

train_data <- train_data %>%
  mutate(page_visits = (page_visits - train_mean) / train_sd) %>%
  select(-index)

index_test <- test_data %>% select(index)

test_data <- test_data %>%
  mutate(page_visits = (page_visits - test_mean) / test_sd) %>%
  select(-index)

# Denormalise
#denormalized_data <- (normalized_data * sd_value) + mean_value

# Create sequences
sequence_length <- 7 

x_train <- list()
x_test <- list()
y_train <- list()
y_test <- list()

for (i in 1:(nrow(train_data) - sequence_length)) {
  x_train[[i]] <- as.matrix(train_data[i:(i + sequence_length - 1),-1])
  y_train[[i]] <- train_data[i + sequence_length, "page_visits"]
  x_test [[i]] <- as.matrix(train_data[i:(i + sequence_length - 1),-1])
  y_test[[i]] <- train_data[i + sequence_length, "page_visits"]
}

# Convert into matrix 
x_train <- array(unlist(x_train), dim = c(length(x_train), sequence_length, ))
y_train <- as.array(y_train)
y_test <- as.array(y_test)

```




```{r}
create_sequences <- function(data, n_steps) {
  x <- NULL
  y <- NULL
  
  data_mat <- as.matrix(data)
  
  for(i in 1:(nrow(data) - n_steps)) {
    x <- rbind(x, data_mat[i:(i + n_steps - 1), ])
    y <- c(y, data_mat[i + n_steps, 1])
  }
  
  list(x = array(x, dim = c((nrow(x) / n_steps), n_steps, ncol(x))),
       y = y)
}

# Example usage:
n_steps <- 10 
sequences <- create_sequences(final_data, n_steps)

# Split data into training and test sets
train_size <- floor(0.8 * length(sequences$y))
x_train <- sequences$x[1:train_size, ,]
y_train <- sequences$y[1:train_size]
x_test <- sequences$x[(train_size + 1):length(sequences$y), ,]
y_test <- sequences$y[(train_size + 1):length(sequences$y)]

y_train_sd <- sd(y_train)
y_train_mean <- mean(y_train)

```

```{r}
x_train_norm <- scale(x_train)
x_test_norm <- scale(x_test)
dim(x_train_norm)
dim(x_test_norm)
```


```{r}
# Intialise the list for comparision between 2 models
model_list <- list()

# Define LSTM model
model_list[["Model 1"]]$model <- keras_model_sequential() %>%
  layer_lstm(units = 50, input_shape = c(10, 886), return_sequences = TRUE) %>%
  layer_lstm(units = 50, return_sequences = TRUE) %>%
  layer_lstm(units = 50) %>%
  layer_dense(units = 1)

# Compile the model
model_list[["Model 1"]]$model %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.001),
  loss = "mse"
)

# Train the model
model_list[["Model 1"]]$fit <- model_list[["Model 1"]]$model %>%
  fit(
    x_train_norm, y_train,
    epochs = 100,
    batch_size = 32,
    validation_data = list(x_test_norm, y_test)
)

# Evaluate the model
model_list[["Model 1"]]$model %>% evaluate(x_test_norm, y_test)

# Make predictions
predictions <- model_list[["Model 1"]]$model %>% predict(x_test_norm)
```



```{r}
# Extract training history
history_data <- as.data.frame(history$metrics)

# Generate epoch vector
epochs <- seq(1, nrow(history_data))

# Plotting Loss
p1 <- ggplot(history_data, aes(x = epochs)) +
  geom_line(aes(y = loss, col = "Training")) +
  geom_line(aes(y = val_loss, col = "Validation")) +
  labs(title = "Model Loss", x = "Epochs", y = "Loss") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()


p1
```


```{r}
# Ensure predictions are the same length as y_test
if(length(predictions) != length(y_test)) {
  stop("Predictions and actual values have different lengths!")
}

# Compute Metrics
mae <- mean(abs(predictions - y_test))  # Mean Absolute Error
mse <- mean((predictions - y_test)^2)    # Mean Squared Error
rmse <- sqrt(mse)                        # Root Mean Squared Error

# Print Metrics
cat("Mean Absolute Error: ", mae, "\n")
cat("Mean Squared Error: ", mse, "\n")
cat("Root Mean Squared Error: ", rmse, "\n")
```

```{r}
cat("Denormalised Mean Absolute Error: ", (mae * y_train_sd) + y_train_mean, "\n")
cat("Denormalised Mean Squared Error: ", (mse * y_train_sd) + y_train_mean, "\n")
cat("Denormalised Root Mean Squared Error: ", (rmse * y_train_sd) + y_train_mean, "\n")
```


```{r}
model_list[["Model 2"]]$model <- keras_model_sequential() %>%
  layer_gru(units = 32, 
            dropout = 0.1, 
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = c(10, 886)) %>% 
  layer_gru(units = 64, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

# Compile the model
model_list[["Model 2"]]$model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mse"
)

# Train the model
model_list[["Model 2"]]$fit <- model_list[["Model 2"]]$model %>% fit(
  x_train_norm, y_train,
  epochs = 100,
  batch_size = 32,
  validation_data = list(x_test_norm, y_test)
)
```


```{r}
# Make predictions
pred_1 <- model_list[["Model 1"]]$model %>% predict(x_test_norm)
pred_2 <- model_list[["Model 2"]]$model %>% predict(x_test_norm)
```

```{r}
# Compute Metrics
mae3 <- mean(abs(predictions3 - y_test))  # Mean Absolute Error
mse3 <- mean((predictions3 - y_test)^2)    # Mean Squared Error
rmse3 <- sqrt(mse3)                        # Root Mean Squared Error

# Print Metrics
cat("Mean Absolute Error: ", mae3, "\n")
cat("Mean Squared Error: ", mse3, "\n")
cat("Root Mean Squared Error: ", rmse3, "\n")
```

```{r}
# Evaluate the model
evaluate_df <- function(model){
  evaluate(model, x_test_norm, y_test, verbose = 0) %>%
    bind_rows()
}

# Convert list to data frame 
model_df <- data_frame(
  name = fct_inorder(names(model_list)),
  model = map(model_list, "model"),
  fit = map(model_list,"fit")) %>%
  mutate(eval = map(model, evaluate_df)) %>%
  unnest(eval)

# Loss and accuracy plot
model_df %>%
  mutate(metrics = map(fit, function(x){data.frame(x)})) %>%
  select(name, metrics) %>%
  unnest(metrics) %>%
  ggplot(aes(x = epoch, y = value, colour = data)) +
  geom_line() +
  facet_grid(metric~name, scales = "free_y")

# Loss and accuracy summary
model_df %>%
  select(name, loss, mae)

# Compare loss and accuracy between models 
model_df %>%
  select(name, loss, mae) %>%
  gather(var,val, - name) %>%
  ggplot(aes(x = name, y = val)) +
  geom_col(colour = "blue", fill = "blue", alpha = 0.2, width = 0) +
  geom_point(colour = "blue") +
  facet_wrap(~var, scales = "free_x") +
  coord_flip()


```




