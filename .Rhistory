coord_flip() +
theme_minimal() +
theme(legend.position = "bottom") +
guides(fill = guide_legend(title = NULL))
# Summarize data
brand_counts <- joined_df %>%
group_by(companyName, deviceCommDistributionStatus) %>%
summarise(unique_brands = n_distinct(brandName)) %>%
arrange(companyName, -unique_brands)
# Plot bar chart
ggplot(brand_counts, aes(x = reorder(companyName, -unique_brands), y = unique_brands, fill = deviceCommDistributionStatus)) +
geom_col(position = "dodge") +
labs(title = "Number of Unique REBOA Brands per Company", x = "Company", y = "Number of Unique Brands") +
coord_flip() +
theme_minimal() +
theme(legend.position = "bottom") +
guides(fill = guide_legend(title = NULL))
?write_csv
reboa_df <- read_csv("data/reboa_filter.csv")
joined_df <- unique_device_df %>%
inner_join(reboa_df, by = c("brandName" = "device name"))
write_csv(joined_df, "reboa_data.csv")
joined_df %>%
ggplot(aes(x = deviceCommDistributionStatus)) +
geom_bar() +
theme_minimal() +
labs(title = "Distribution of Device Commercial Distribution Status",
x = "Distribution Status",
y = "Count")
# Histogram of publication dates
joined_df %>%
ggplot(aes(x = as.Date(devicePublishDate))) +
geom_histogram(binwidth=30) + # Monthly bins
theme_minimal() +
labs(title = "Histogram of Device Publication Dates",
x = "Publication Date",
y = "Count")
# Bar chart of record statuses
joined_df %>%
ggplot(aes(x = deviceRecordStatus)) +
geom_bar() +
theme_minimal() +
labs(title = "Distribution of Device Record Statuses",
x = "Record Status",
y = "Count")
# Bar chart of version statuses
joined_df %>%
ggplot(aes(x = publicVersionStatus)) +
geom_bar() +
theme_minimal() +
labs(title = "Distribution of Public Version Statuses",
x = "Version Status",
y = "Count")
joined_df %>%
ggplot(aes(x = deviceCommDistributionStatus, fill = deviceRecordStatus)) +
geom_bar(position = "dodge") +
theme_minimal() +
labs(title = "Distribution of Device Commercial Distribution Status by Record Status",
x = "Distribution Status",
y = "Count") +
theme(legend.title = element_text("Record Status"))
# Histogram of publication dates grouped by version status
joined_df %>%
ggplot(aes(x = as.Date(devicePublishDate), fill = publicVersionStatus)) +
geom_histogram(binwidth=30, position = "identity", alpha = 0.6) +
theme_minimal() +
labs(title = "Histogram of Device Publication Dates by Version Status",
x = "Publication Date",
y = "Count") +
theme(legend.title = element_text("Version Status"))
# Histogram of publication dates grouped by version status
joined_df %>%
ggplot(aes(x = as.Date(devicePublishDate), fill = publicVersionStatus)) +
geom_histogram(binwidth=30, position = "identity", alpha = 0.6) +
theme_minimal() +
labs(title = "Histogram of Device Publication Dates by Version Status",
x = "Publication Date",
y = "Count") +
theme(legend.title = element_text("Version Status"))
# Bar chart of top brands
top_brands <- joined_df %>%
count(brandName) %>%
top_n(10, wt=n) %>%
pull(brandName)
joined_df %>%
filter(brandName %in% top_brands) %>%
ggplot(aes(x = brandName)) +
geom_bar() +
theme_minimal() +
labs(title = "Top 10 Device Brands by Count",
x = "Brand Name",
y = "Count") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Bar chart of top brands
# Bar chart of distribution status grouped by device sterility
joined_df %>%
ggplot(aes(x = deviceCommDistributionStatus, fill = as.factor(deviceSterile))) +
geom_bar(position = "dodge") +
theme_minimal() +
labs(title = "Distribution of Device Commercial Distribution Status by Sterility",
x = "Distribution Status",
y = "Count",
fill = "Device Sterile")
reboa_df <- read_csv("data/reboa_filter.csv")
joined_df <- data %>%
inner_join(reboa_df, by = c("brandName" = "device name"))
#write_csv(joined_df, "reboa_data.csv")
brands_filterd <- joined_df %>%  group_by(companyName) %>% select(brandName,companyName) %>% distinct()
brands %>% kable()
library("tidyverse")
library("kableExtra")
library("ggplot2")
library("lubridate")
install.packages("kableExtra")
library("tidyverse")
library("kableExtra")
library("ggplot2")
library("lubridate")
library("kableExtra")
companies <- data %>% select(companyName) %>% distinct()
companies %>% kable()
install.packages(c("knitr", "rmarkdown", "stringr", "magrittr", "clipr", "huxtable", "readr", "forcats"))
install.packages(c("knitr", "rmarkdown", "stringr", "magrittr", "clipr", "huxtable", "readr", "forcats"))
library("tidyverse")
library("kableExtra")
library("ggplot2")
library("lubridate")
brands_filterd <- joined_df %>%  group_by(companyName) %>% select(brandName,companyName) %>% distinct()
brands_filterd %>% kable()
# Load necessary libraries
library(dplyr)
library(readr)
# Load the datasets
wikipedia <- read_csv("wikipedia.csv")
train_1 <- read_csv("train_1.csv")
train_2 <- read_csv("train_2.csv")
# Extract unique page titles from wikipedia.csv
wiki_titles <- unique(wikipedia$Page)
# Filter train_1 and train_2 based on these titles
filtered_train_1 <- train_1 %>% filter(Page %in% wiki_titles)
filtered_train_2 <- train_2 %>% filter(Page %in% wiki_titles)
# Load necessary libraries
library(dplyr)
library(readr)
# Load the datasets
wikipedia <- read_csv("wikipedia.csv")
train_1 <- read_csv("train_1.csv")
train_2 <- read_csv("train_2.csv")
# Extract unique page titles from wikipedia.csv
wiki_titles <- unique(wikipedia$Page)
# Filter train_1 and train_2 based on these titles
filtered_train_1 <- train_1 %>% filter(Page %in% wiki_titles)
filtered_train_2 <- train_2 %>% filter(Page %in% wiki_titles)
View(filtered_train_1)
View(train_2)
View(train_1)
View(train_2)
install.packages("VennDiagram")
# Create a list of titles from each dataset
wiki_titles_set <- unique(wikipedia$Page)
train_1_titles_set <- unique(train_1$Page)
train_2_titles_set <- unique(train_2$Page)
# Draw a Venn diagram
venn.plot <- draw.triple.venn(
category1 = "Wikipedia",
category2 = "Train 1",
category3 = "Train 2",
area1 = length(wiki_titles_set),
area2 = length(train_1_titles_set),
area3 = length(train_2_titles_set),
n12 = length(intersect(wiki_titles_set, train_1_titles_set)),
n23 = length(intersect(train_1_titles_set, train_2_titles_set)),
n13 = length(intersect(wiki_titles_set, train_2_titles_set)),
n123 = length(intersect(intersect(wiki_titles_set, train_1_titles_set), train_2_titles_set)),
output=TRUE
)
library(VennDiagram)
# Create a list of titles from each dataset
wiki_titles_set <- unique(wikipedia$Page)
train_1_titles_set <- unique(train_1$Page)
train_2_titles_set <- unique(train_2$Page)
# Draw a Venn diagram
venn.plot <- draw.triple.venn(
category1 = "Wikipedia",
category2 = "Train 1",
category3 = "Train 2",
area1 = length(wiki_titles_set),
area2 = length(train_1_titles_set),
area3 = length(train_2_titles_set),
n12 = length(intersect(wiki_titles_set, train_1_titles_set)),
n23 = length(intersect(train_1_titles_set, train_2_titles_set)),
n13 = length(intersect(wiki_titles_set, train_2_titles_set)),
n123 = length(intersect(intersect(wiki_titles_set, train_1_titles_set), train_2_titles_set)),
output=TRUE
)
write_csv(filtered_train_1, "filtered_train_1.csv")
write_csv(filtered_train_2, "filtered_train_2.csv")
View(filtered_train_1)
# Load necessary libraries
library(dplyr)
library(readr)
library(VennDiagram)
# Load the datasets
wikipedia <- read_csv("wikipedia.csv")
train_1 <- read_csv("train_1.csv")
train_2 <- read_csv("train_2.csv")
# Extract unique page titles from wikipedia.csv
wiki_titles <- unique(wikipedia$Page)
# Filter train_1 and train_2 based on these titles
filtered_train_1 <- train_1 %>% filter(Page %in% wiki_titles)
filtered_train_2 <- train_2 %>% filter(Page %in% wiki_titles)
View(wikipedia)
# Load the datasets
wikipedia <- read_csv("wikipedia.csv", header=FALSE)
# Load the datasets
wikipedia <- read_csv("wikipedia.csv", header=FALSE)
# Load the datasets
wikipedia <- read_csv("wikipedia.csv", col_names=FALSE)
# Load necessary libraries
library(dplyr)
library(readr)
library(VennDiagram)
# Load the datasets
wikipedia <- read_csv("wikipedia.csv", col_names=FALSE)
train_1 <- read_csv("train_1.csv")
train_2 <- read_csv("train_2.csv")
# Extract unique page titles from wikipedia.csv
wiki_titles <- unique(X1$Page)
# Extract unique page titles from wikipedia.csv
wiki_titles <- unique(wikipedia$X1)
# Load necessary libraries
library(dplyr)
library(readr)
library(VennDiagram)
# Load the datasets
wikipedia <- read_csv("wikipedia.csv", col_names=FALSE)
train_1 <- read_csv("train_1.csv")
train_2 <- read_csv("train_2.csv")
# Extract unique page titles from wikipedia.csv
wiki_titles <- unique(wikipedia$X1)
# Filter train_1 and train_2 based on these titles
filtered_train_1 <- train_1 %>% filter(Page %in% wiki_titles)
filtered_train_2 <- train_2 %>% filter(Page %in% wiki_titles)
write_csv(filtered_train_1, "filtered_train_1.csv")
write_csv(filtered_train_2, "filtered_train_2.csv")
install.packages("fpp3")
# 2. Reshape the data to a "long" format
train_1_long <- filtered_train_1 %>%
gather(key = "Date", value = "Views", -Page)
library(tidyverse)
# 2. Reshape the data to a "long" format
train_1_long <- filtered_train_1 %>%
gather(key = "Date", value = "Views", -Page)
train_2_long <- filtered_train_2 %>%
gather(key = "Date", value = "Views", -Page)
# 3. Plot the data
# For train_1
ggplot(train_1_long, aes(x = Date, y = Views, group = Page)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Train 1", x = "Date", y = "Views")
# For train_2
ggplot(train_2_long, aes(x = Date, y = Views, group = Page)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Train 2", x = "Date", y = "Views")
# 2. Reshape the data to a "long" format
train_1_long <- filtered_train_1 %>%
gather(key = "Date", value = "Views", -Page)
train_2_long <- filtered_train_2 %>%
gather(key = "Date", value = "Views", -Page)
# 3. Plot the data
# For train_1
ggplot(train_1_long, aes(x = Date, y = Views, group = Page)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Train 1", x = "Date", y = "Views")
# For train_2
ggplot(train_2_long, aes(x = Date, y = Views, group = Page)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Train 2", x = "Date", y = "Views")
# 2. Reshape the data to a "long" format
train_1_long <- filtered_train_1 %>%
gather(key = "Date", value = "Views", -Page)
train_2_long <- filtered_train_2 %>%
gather(key = "Date", value = "Views", -Page)
wikipedia_long <- wikipedia %>%
gather(key = "Date", value = "Views", -`X1`)
# 3. Plot the data
# For train_1
ggplot(train_1_long, aes(x = Date, y = Views, group = Page)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Train 1", x = "Date", y = "Views")
# For train_2
ggplot(train_2_long, aes(x = Date, y = Views, group = Page)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Train 2", x = "Date", y = "Views")
# Plot the wikipedia data
ggplot(wikipedia_long, aes(x = Date, y = Views, group = `X1`)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Wikipedia Data", x = "Date", y = "Views")
wiki_views <- filter(wikipedia_long, `Michael_J._Fox_en.wikipedia.org_all-access_all-agents` == 'PageName')$Views
View(train_2_long)
wiki_views <- filter(wikipedia_long, `Emmanuel_Macron_fr.wikipedia.org_desktop_all-agents` == 'PageName')$Views
wiki_views <- filter(wikipedia_long, `Emmanuel_Macron_fr.wikipedia.org_desktop_all-agents ` == 'PageName')$Views
wiki_views <- filter(wikipedia_long, `Emmanuel_Macron_fr.wikipedia.org_desktop_all-agents` == 'PageName')$Views
View(wikipedia_long)
wiki_views <- filter(wikipedia_long, `Michael_J._Fox_en.wikipedia.org_all-access_all-agents` == 'PageName')$Views
wiki_views <- filter(wikipedia_long, `Michael_J._Fox_en.wikipedia.org_all-access_all-agents ` == 'PageName')$Views
wiki_views <- filter(wikipedia_long, `Michael_J._Fox_en.wikipedia.org_all-access_all-agents` == 'PageName')$Views
wikipedia_long$`Michael_J._Fox_en.wikipedia.org_all-access_all-agents`
# Assuming you've reshaped the wikipedia.csv data into wikipedia_long
# For demonstration, let's take a single page's views from both datasets
# Make sure to replace 'PageName' with an actual page name common to both datasets
wiki_views <- filter(wikipedia_long, X1 == 'PageName')$Views
train_2_views <- filter(train_2_long, Page == 'PageName')$Views
# Compute the cross-correlation
ccf_result <- ccf(wiki_views, train_2_views, lag.max = 365, plot = TRUE)
# Identify common pages
common_pages <- intersect(unique(wikipedia$X1), unique(train_2$Page))
# Take the first common page as an example
page_name <- common_pages[1]
# Extract views for the chosen page from both datasets
wiki_views <- filter(wikipedia_long, X1 == page_name)$Views
train_2_views <- filter(train_2_long, Page == page_name)$Views
# Compute the cross-correlation
ccf_result <- ccf(wiki_views, train_2_views, lag.max = 365, plot = TRUE)
# Get the lag with the highest correlation
best_lag <- which.max(ccf_result$acf)
# Print the best lag
best_lag
# Identify common pages
common_pages <- intersect(unique(wikipedia$X1), unique(train_2$Page))
# Take the first common page as an example
page_name <- common_pages[1]
# Extract views for the chosen page from both datasets
wiki_views <- filter(wikipedia_long, X1 == page_name)$Views
train_2_views <- filter(train_2_long, Page == page_name)$Views
# Compute the cross-correlation
ccf_result <- ccf(wiki_views, train_2_views, lag.max = 400, plot = TRUE)
# Get the lag with the highest correlation
best_lag <- which.max(ccf_result$acf)
# Print the best lag
best_lag
# Identify common pages
common_pages <- intersect(unique(wikipedia$X1), unique(train_2$Page))
# Take the first common page as an example
page_name <- common_pages[1]
# Extract views for the chosen page from both datasets
wiki_views <- filter(wikipedia_long, X1 == page_name)$Views
train_2_views <- filter(train_2_long, Page == page_name)$Views
# Compute the cross-correlation
ccf_result <- ccf(wiki_views, train_2_views, lag.max = 700, plot = TRUE)
# Get the lag with the highest correlation
best_lag <- which.max(ccf_result$acf)
# Print the best lag
best_lag
# Identify common pages
common_pages <- intersect(unique(wikipedia$X1), unique(train_2$Page))
# Take the first common page as an example
page_name <- common_pages[1]
# Extract views for the chosen page from both datasets
wiki_views <- filter(wikipedia_long, X1 == page_name)$Views
train_2_views <- filter(train_2_long, Page == page_name)$Views
# Compute the cross-correlation
ccf_result <- ccf(wiki_views, train_2_views, lag.max = 365, plot = TRUE)
# Get the lag with the highest correlation
best_lag <- which.max(ccf_result$acf)
# Print the best lag
best_lag
train_2_long <- filtered_train_2 %>%
gather(key = "Date", value = "Views", -Page)
wikipedia_long <- wikipedia %>%
gather(key = "Date", value = "Views", -`X1`)
# For train_2
ggplot(train_2_long, aes(x = Date, y = Views, group = Page)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Train 2", x = "Date", y = "Views")
# Plot the wikipedia data
ggplot(wikipedia_long, aes(x = Date, y = Views, group = `X1`)) +
geom_line(alpha = 0.1) +
labs(title = "Time Series for Wikipedia Data", x = "Date", y = "Views")
View(wikipedia_long)
install.packages("keras")
# Load necessary libraries
library(dplyr)
library(readr)
library(tidyverse)
library(urca)
library(fpp3)
# Load the datasets
wiki_df <- read_csv('wikipedia.csv', col_names=FALSE)
train_df <- read_csv('filtered_train_2.csv')
# Extract the first time series from both datasets
wiki_series <- as.numeric(wiki_df[2, -1])
train_series <- as.numeric(train_df[2, -1])
# Normalize the series
wiki_series <- scale(wiki_series)
train_series <- scale(train_series)
# Initialize variables to store maximum correlation and its corresponding lag
max_corr <- -1
max_lag <- 0
# Loop over possible lags and calculate correlation for each lag
for (lag in 0:(min(length(wiki_series), length(train_series)) - 1)) {
wiki_subseries <- wiki_series[(lag + 1):(min(length(wiki_series), length(train_series) + lag))]
train_subseries <- train_series[1:(min(length(wiki_series) - lag, length(train_series)))]
# Ensure the lengths of the series are equal before calculating the correlation
if(length(wiki_subseries) == length(train_subseries)) {
# Calculate correlation for the current lag
current_corr <- cor(wiki_subseries, train_subseries)
# Update maximum correlation and its corresponding lag if the current correlation is greater
if (current_corr > max_corr) {
max_corr <- current_corr
max_lag <- lag
}
}
}
# Calculate the start date for the Wikipedia series based on the optimal lag
start_date <- as.Date(names(train_df)[2]) + max_lag
# Generate the date range for the Wikipedia series
wiki_dates <- seq(start_date, by='day', length.out=ncol(wiki_df) - 1)  # Correcting the length of the date sequence
# Create a new DataFrame for the Wikipedia dataset with the estimated dates as columns
wiki_df_with_dates <- as.data.frame(cbind(Page=wiki_df$X1, as.data.frame(matrix(unlist(wiki_df[-1]), nrow=nrow(wiki_df), byrow=T))))
# Assign the generated dates as column names
colnames(wiki_df_with_dates)[-1] <- as.character(wiki_dates)
# View the resulting DataFrame
head(wiki_df_with_dates)
# Load necessary libraries
library(dplyr)
library(readr)
library(tidyverse)
library(urca)
library(fpp3)
# Load the datasets
wiki_df <- read_csv('wikipedia.csv', col_names=FALSE)
train_df <- read_csv('filtered_train_2.csv')
# Extract the first time series from both datasets
wiki_series <- as.numeric(wiki_df[2, -1])
train_series <- as.numeric(train_df[2, -1])
# Normalize the series
wiki_series <- scale(wiki_series)
train_series <- scale(train_series)
# Initialize variables to store maximum correlation and its corresponding lag
max_corr <- -1
max_lag <- 0
# Loop over possible lags and calculate correlation for each lag
for (lag in 0:(min(length(wiki_series), length(train_series)) - 1)) {
wiki_subseries <- wiki_series[(lag + 1):(min(length(wiki_series), length(train_series) + lag))]
train_subseries <- train_series[1:(min(length(wiki_series) - lag, length(train_series)))]
# Ensure the lengths of the series are equal before calculating the correlation
if(length(wiki_subseries) == length(train_subseries)) {
# Calculate correlation for the current lag
current_corr <- cor(wiki_subseries, train_subseries)
# Update maximum correlation and its corresponding lag if the current correlation is greater
if (current_corr > max_corr) {
max_corr <- current_corr
max_lag <- lag
}
}
}
# Calculate the start date for the Wikipedia series based on the optimal lag
start_date <- as.Date(names(train_df)[2]) + max_lag
# Generate the date range for the Wikipedia series
wiki_dates <- seq(start_date, by='day', length.out=ncol(wiki_df) - 1)  # Correcting the length of the date sequence
# Create a new DataFrame for the Wikipedia dataset with the estimated dates as columns
wiki_df_with_dates <- as.data.frame(cbind(Page=wiki_df$X1, as.data.frame(matrix(unlist(wiki_df[-1]), nrow=nrow(wiki_df), byrow=T))))
# Assign the generated dates as column names
colnames(wiki_df_with_dates)[-1] <- as.character(wiki_dates)
# View the resulting DataFrame
head(wiki_df_with_dates)
wiki_long <- wiki_df_with_dates %>%
pivot_longer(cols = -Page, names_to = "Date", values_to = "Traffic") %>%
mutate(Date = as.Date(Date, format="%Y-%m-%d"))
# Initialize a list to store the forecasts for each page
page_forecasts <- list()
# Loop through each unique page in the dataset
for (page in unique(wiki_long$Page)) {
# Filter the data for the current page
page_data <- wiki_long %>%
filter(Page == page) %>%
select(-Page) %>%
as_tsibble(index = Date)
# Check if there are sufficient non-NA values to fit the model
if (sum(!is.na(page_data$Traffic)) > 2) {
# Fit the ARIMA model and forecast
forecast <- page_data %>%
model(ARIMA(Traffic)) %>%
forecast(h = forecast_horizon)
# Store the forecasts in the list
page_forecasts[[page]] <- forecast
}
}
forecast_horizon <- 30
wiki_long <- wiki_df_with_dates %>%
pivot_longer(cols = -Page, names_to = "Date", values_to = "Traffic") %>%
mutate(Date = as.Date(Date, format="%Y-%m-%d"))
# Initialize a list to store the forecasts for each page
page_forecasts <- list()
# Loop through each unique page in the dataset
for (page in unique(wiki_long$Page)) {
# Filter the data for the current page
page_data <- wiki_long %>%
filter(Page == page) %>%
select(-Page) %>%
as_tsibble(index = Date)
# Check if there are sufficient non-NA values to fit the model
if (sum(!is.na(page_data$Traffic)) > 2) {
# Fit the ARIMA model and forecast
forecast <- page_data %>%
model(ARIMA(Traffic)) %>%
forecast(h = forecast_horizon)
# Store the forecasts in the list
page_forecasts[[page]] <- forecast
}
}
