---
title: "wikipida"
author: "Aswin Kithalawaarachchi"
date: "2023-09-20"
output: html_document
---

```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(tidyverse)



# Load the datasets
wikipedia <- read_csv("wikipedia.csv", col_names=FALSE)


```
```{r}

filtered_train_1 <- read_csv("filtered_train_1.csv")
filtered_train_2 <- read_csv("filtered_train_2.csv")


train_2_long <- filtered_train_2 %>% 
  gather(key = "Date", value = "Views", -Page)

wikipedia_long <- wikipedia %>%
  gather(key = "Date", value = "Views", -`X1`)




# For train_2
ggplot(train_2_long, aes(x = Date, y = Views, group = Page)) + 
  geom_line(alpha = 0.1) +
  labs(title = "Time Series for Train 2", x = "Date", y = "Views")



# Plot the wikipedia data
ggplot(wikipedia_long, aes(x = Date, y = Views, group = `X1`)) + 
  geom_line(alpha = 0.1) +
  labs(title = "Time Series for Wikipedia Data", x = "Date", y = "Views")

```

```{r get the country/device}
country_class <- function (name) {
  name_and_country_code <- str_split(name, "[.]")[1]
  thingies <- str_split(name_and_country_code, "_")
  token_len <- len(thingies)
  
  return (thingies[token_len])
}

device_class <- function (name) {
  device_code <- str_split(name, "[.]")[2]
  
  return (device_code)
}
```


```{r}


# Identify common pages
common_pages <- intersect(unique(wikipedia$X1), unique(train_2$Page))

# Take the first common page as an example
page_name <- common_pages[1]

# Extract views for the chosen page from both datasets
wiki_views <- filter(wikipedia_long, X1 == page_name)$Views
train_2_views <- filter(train_2_long, Page == page_name)$Views

# Compute the cross-correlation
ccf_result <- ccf(wiki_views, train_2_views, lag.max = 365, plot = TRUE)

# Get the lag with the highest correlation
best_lag <- which.max(ccf_result$acf)

# Print the best lag
best_lag

```

