---
title: "wikipida"
author: "Aswin Kithalawaarachchi"
date: "2023-09-20"
output: html_document
---

```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(tidyverse)
library(ggplot2)
library(urca)
library(fpp3)
library(fable)
library(naniar)
library(future)
library(furrr)
library(keras)
library(zoo)
library(purrr)
library(caret)
```


```{r get the language / device}
get_language <- function (name) {
  name_and_language_codes <- str_split(name, "[.]wikipedia[.]org") %>%
    map_chr(first)
  language_codes <- str_split(name_and_language_codes, "_") %>%
    map_chr(last)
  
  return (language_codes)
}

get_device <- function (name) {
  str_split(name, "[.]wikipedia[.]org_") %>%
    map_chr(last)
}

get_name <- function (name) {
  name_and_country_codes <- str_split(name, "[.]wikipedia[.]org") %>%
    map(first)
  name <- gsub('.{3}$', '', name_and_country_codes)
  
  return (name)
  
}
```


```{r}
# Load the datasets
wiki_df <- read_csv('wikipedia.csv', col_names=FALSE)
train_df <- read_csv('filtered_train_2.csv')

# Extract the first time series from both datasets
wiki_series <- as.numeric(wiki_df[2, -1])
train_series <- as.numeric(train_df[2, -1])

# Normalize the series
wiki_series <- scale(wiki_series)
train_series <- scale(train_series)

# Initialize variables to store maximum correlation and its corresponding lag
max_corr <- -1
max_lag <- 0

# Loop over possible lags and calculate correlation for each lag
for (lag in 0:(min(length(wiki_series), length(train_series)) - 1)) {
  wiki_subseries <- wiki_series[(lag + 1):(min(length(wiki_series), length(train_series) + lag))]
  train_subseries <- train_series[1:(min(length(wiki_series) - lag, length(train_series)))]
  
  # Ensure the lengths of the series are equal before calculating the correlation
  if(length(wiki_subseries) == length(train_subseries)) {
    # Calculate correlation for the current lag
    current_corr <- cor(wiki_subseries, train_subseries)
    
    # Update maximum correlation and its corresponding lag if the current correlation is greater
    if (current_corr > max_corr) {
      max_corr <- current_corr
      max_lag <- lag
    }
  }
}

# Calculate the start date for the Wikipedia series based on the optimal lag
start_date <- as.Date(names(train_df)[2]) + max_lag

# Generate the date range for the Wikipedia series
wiki_dates <- seq(start_date, by='day', length.out=ncol(wiki_df) - 1)  # Correcting the length of the date sequence

# Create a new DataFrame for the Wikipedia dataset with the estimated dates as columns
wiki_df_with_dates <- as.data.frame(
  cbind(
    Page=wiki_df$X1,
    as.data.frame(
      matrix(
        unlist(wiki_df[-1]),
        nrow=nrow(wiki_df),
        byrow=T)
      )
    )
  )

# Assign the generated dates as column names
colnames(wiki_df_with_dates)[-1] <- as.character(wiki_dates)

# View the resulting DataFrame
head(wiki_df_with_dates)

```

```{r remove non-wikipedia sites}
wikipedia_site_regex <- "[.]wikipedia[.]org"

filtered_wiki_df_with_dates <- wiki_df_with_dates %>% 
  filter(str_detect(Page, wikipedia_site_regex))
```

```{r add country/device}
pages <- filtered_wiki_df_with_dates$Page

names <- get_name(pages)

languages <- get_language(pages)

devices <- get_device(pages)

wiki_df_with_dates_country_device <- filtered_wiki_df_with_dates %>% 
  mutate(name = names, language = languages, device = devices) %>%
  relocate(c(name, language, device), .after=Page)
```

```{r remove spider pages}
filtered_wiki_df_with_dates_country_device <- wiki_df_with_dates_country_device %>% 
  filter(device != "all-access_spider")
head(filtered_wiki_df_with_dates_country_device)
```

```{r}

Avg_Web_Traffic <- filtered_wiki_df_with_dates_country_device %>%
  filter(Page == "Michael_J._Fox_en.wikipedia.org_all-access_all-agents") %>%
  pivot_longer(cols = -c(Page, name, language, device), names_to = 'Date', values_to = 'Web_Traffic') %>%
  mutate(Date = as.Date(Date, format = '%Y-%m-%d')) %>%
  group_by(Date) %>%
  summarise(Avg_Web_Traffic = mean(Web_Traffic))

head(Avg_Web_Traffic)
nrow(Avg_Web_Traffic)

Avg_Web_Traffic %>%
  ggplot() +
  geom_line(aes(x = Date, y = Avg_Web_Traffic))

```

```{r}
# Melting the data to have it in a long format suitable for time series analysis
data_long <- wiki_df_with_dates_country_device %>% 
  pivot_longer(cols = starts_with("2017")|starts_with("2018")|starts_with("2019")|starts_with("2020"),
               names_to = "date", values_to = "page_visits") %>% 
  mutate(date = as.Date(date, format = "%Y-%m-%d"))


  
```


```{r}
ets_comps <- read_csv("data_with_ETS_components.csv")
data_long_unique <- data_long %>% distinct(Page, .keep_all = T) %>% select(-date,-page_visits)

ets_comp_joined_df <- data_long %>%
  distinct(Page, .keep_all = TRUE) %>%
  select(-date, -page_visits) %>%
  left_join(ets_comps, by = "Page") %>%
  mutate(ETS_content = str_extract(`ETS(clicks)`, "(?<=ETS\\()[A-Z,]+(?=\\))")) %>%
  separate(ETS_content, into = c("error", "trend", "seasonality"), sep = ",") %>%
  select(-`ETS(clicks)`)
head(ets_comp_joined_df)
dim(ets_comp_joined_df)
```





```{r average ETS and ARIMA}

# Example data: replace this part with your actual data loading
# mydata <- data.frame(date=..., value=...)

# Convert the data frame into a tsibble object
Avg_Web_Traffic_tsibble <- Avg_Web_Traffic %>%
  as_tsibble(index = Date)

# Perform train-test split: 70% train, 30% test
split_index <- floor(0.70 * nrow(Avg_Web_Traffic_tsibble))
train_data <- Avg_Web_Traffic_tsibble[1:split_index,]
test_data <- Avg_Web_Traffic_tsibble[(split_index+1):nrow(Avg_Web_Traffic_tsibble),]

# Fit ARIMA and ETS models
fit_arima <- train_data %>%
  model(ARIMA(Avg_Web_Traffic))
  
fit_ets <- train_data %>%
  model(ETS(Avg_Web_Traffic))

# Forecast
h <- nrow(test_data)  # forecast horizon is the size of the test set

forecast_arima <- fit_arima %>% 
  forecast(h = h)
  
forecast_ets <- fit_ets %>% 
  forecast(h = h)

# Calculate RMSE and other accuracy measures
accuracy_arima <- accuracy(forecast_arima, test_data)
accuracy_ets <- accuracy(forecast_ets, test_data)

# Check and compare accuracy measures
accuracy_arima
accuracy_ets

```


```{r}
# Data Visualization purpose
Avg_Web_Traffic_with_yearmonth <- Avg_Web_Traffic%>%
  mutate(Year = as.numeric(format(Date,'%Y')),
         Month = as.numeric(format(Date,'%m ')))

Avg_Web_Traffic_with_yearmonth %>%
  ggplot(aes(x = Month, y = Avg_Web_Traffic)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~ Year, nrow = 1)

```


```{r}
# Maybe don't need this code chunk 
df_long <- filtered_wiki_df_with_dates_country_device %>% pivot_longer(
  cols = -c(Page, name, language, device),
  names_to = "date",
  values_to = "clicks"
)

head(df_long)

n_dates <- nrow(filtered_wiki_df_with_dates_country_device) - 4
training_split_index <- ceiling(0.7 * n_dates)
  
training_split <- df_long %>% group_by(name, country, device) %>% 
  slice(1:training_split_index)

testing_split <- df_long %>% group_by(name, country, device) %>% 
  slice(training_split_index + 1: n_dates)
```


```{r}
# Maybe don't need this code chunk 
training_wide <- training_split %>% pivot_wider(names_from = date, values_from = clicks)

testing_wide <- testing_split %>% pivot_wider(names_from = date, values_from = clicks)
```


```{r add lags to avg}

ACF(Avg_Web_Traffic_tsibble) %>%
  autoplot()
# spikes in lag 13 and 15 are significant 

# Specify the lag values 
lag_13 <- 13
lag_15 <- 15

Avg_Web_Traffic_lags <- Avg_Web_Traffic_with_yearmonth %>%
  # Create lagged columns for both lag values
  mutate(Avg_Web_Traffic_lag_13 = lag(Avg_Web_Traffic, order_by = Date, n = lag_13),
         Avg_Web_Traffic_lag_15 = lag(Avg_Web_Traffic, order_by = Date, n = lag_15)) %>%
  # Handle missing values by replacing them with 0
  mutate(Avg_Web_Traffic_lag_13 = ifelse(is.na(Avg_Web_Traffic_lag_13), 0, Avg_Web_Traffic_lag_13),
         Avg_Web_Traffic_lag_15 = ifelse(is.na(Avg_Web_Traffic_lag_15), 0, Avg_Web_Traffic_lag_15))

Avg_Web_Traffic_lags
```


```{r Preparing data}

X <- Avg_Web_Traffic_lags[, c("Avg_Web_Traffic_lag_13", "Avg_Web_Traffic_lag_15")]
Y <- Avg_Web_Traffic_lags$Avg_Web_Traffic

# Normalize the input features
X <- scale(X)

# Define the sequence length (e.g., 30 days for a month)
sequence_length <- 30

# Create sequences for training
X_train_seq <- array(0, dim = c(nrow(Avg_Web_Traffic_lags) - sequence_length + 1, sequence_length, ncol(X)))
Y_train_seq <- numeric(nrow(Avg_Web_Traffic_lags) - sequence_length + 1)

for (i in 1:(nrow(Avg_Web_Traffic_lags) - sequence_length + 1)) {
  X_train_seq[i,,] <- X[i:(i + sequence_length - 1), ]
  Y_train_seq[i] <- Y[i + sequence_length - 20]  # Forecasting 20 days ahead
}

# Split the data into training and testing sets
train_size <- 0.8  # 80% for training, 20% for testing
train_samples <- round(train_size * length(Y_train_seq))

X_train <- X_train_seq[1:train_samples, , ]
X_test <- X_train_seq[(train_samples + 1):nrow(X_train_seq), , ]
Y_train <- Y_train_seq[1:train_samples]
Y_test <- Y_train_seq[(train_samples + 1):length(Y_train_seq)]
```


```{r neuron network model 1}

# Intialise the list for comparision between 2 models
model_list <- list()

# ChatGPT
model_list[["Model 1"]]$model <- keras_model_sequential() %>%
  layer_lstm(units = 360, input_shape = c(sequence_length, 2), return_sequences = TRUE) %>%
  layer_dropout(rate = 0.4) %>%
  layer_lstm(units = 150, return_sequences = TRUE) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1, activation = 'tanh')
```
  
```{r compile the models}
model_list[["Model 1"]]$model_compiled <-  model_list[["Model 1"]]$model %>%
  compile(loss = "mean_squared_error",
    optimizer = 'adam',
    metrics = c("accuracy"))
```


```{r train the models}

model_list[["Model 1"]]$fit <- model_list[["Model 1"]]$model %>%
  fit(
  X_train, Y_train,
  epochs = 10,  # You can adjust the number of epochs
  batch_size = 128,  # You can adjust the batch size
  validation_data = list(X_test, Y_test),
  verbose = 2
)

```

```{r evaluation}
evaluate_df <- function(model){
  evaluate(model, X_test, Y_test, verbose = 0) %>%
    bind_rows()
}

# Convert list to data frame 
model_df <- data_frame(
  name = fct_inorder(names(model_list)),
  model = map(model_list, "model"),
  fit = map(model_list, "fit")
) %>%
  mutate(eval = map(model, evaluate_df)) %>%
  unnest(eval)

# Loss and accuracy plot
model_df %>%
  mutate(metrics = map(fit, function(x){data.frame(x)})) %>%
  select(name, metrics) %>%
  unnest(metrics) %>%
  ggplot(aes(x = epoch, y = value, colour = data)) +
  geom_line() +
  facet_grid(metric~name, scales = "free_y")

# Loss and accuracy summary
model_df %>%
  select(name, loss, accuracy) %>%
  knitr::kable(caption = "Loss and Accuracy for 2 models")

# Compare loss and accuracy between models 
model_df %>%
  select(name, loss, accuracy) %>%
  gather(var,val, -name) %>%
  ggplot(aes(x = name, y = val)) +
  geom_col(colour = "blue", fill = "blue", alpha = 0.2, width = 0) +
  geom_point(colour = "blue") +
  facet_wrap(~var, scales = "free_x") +
  coord_flip()

```










