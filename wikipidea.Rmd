---
title: "wikipida"
author: "Aswin Kithalawaarachchi"
date: "2023-09-20"
output: html_document
---

```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(tidyverse)
library(ggplot2)
library(urca)
library(fpp3)
library(fable)
library(naniar)
library(future)
library(furrr)
library(keras)
library(zoo)
library(purrr)
library(caret)
```


```{r get the language / device}
get_language <- function (name) {
  name_and_language_codes <- str_split(name, "[.]wikipedia[.]org") %>%
    map_chr(first)
  language_codes <- str_split(name_and_language_codes, "_") %>%
    map_chr(last)
  
  return (language_codes)
}

get_device <- function (name) {
  str_split(name, "[.]wikipedia[.]org_") %>%
    map_chr(last)
}

get_name <- function (name) {
  name_and_country_codes <- str_split(name, "[.]wikipedia[.]org") %>%
    map(first)
  name <- gsub('.{3}$', '', name_and_country_codes)
  
  return (name)
  
}
```


```{r}
# Load the datasets
wiki_df <- read_csv('wikipedia.csv', col_names=FALSE)
train_df <- read_csv('filtered_train_2.csv')

# Extract the first time series from both datasets
wiki_series <- as.numeric(wiki_df[2, -1])
train_series <- as.numeric(train_df[2, -1])

# Normalize the series
wiki_series <- scale(wiki_series)
train_series <- scale(train_series)

# Initialize variables to store maximum correlation and its corresponding lag
max_corr <- -1
max_lag <- 0

# Loop over possible lags and calculate correlation for each lag
for (lag in 0:(min(length(wiki_series), length(train_series)) - 1)) {
  wiki_subseries <- wiki_series[(lag + 1):(min(length(wiki_series), length(train_series) + lag))]
  train_subseries <- train_series[1:(min(length(wiki_series) - lag, length(train_series)))]
  
  # Ensure the lengths of the series are equal before calculating the correlation
  if(length(wiki_subseries) == length(train_subseries)) {
    # Calculate correlation for the current lag
    current_corr <- cor(wiki_subseries, train_subseries)
    
    # Update maximum correlation and its corresponding lag if the current correlation is greater
    if (current_corr > max_corr) {
      max_corr <- current_corr
      max_lag <- lag
    }
  }
}

# Calculate the start date for the Wikipedia series based on the optimal lag
start_date <- as.Date(names(train_df)[2]) + max_lag

# Generate the date range for the Wikipedia series
wiki_dates <- seq(start_date, by='day', length.out=ncol(wiki_df) - 1)  # Correcting the length of the date sequence

# Create a new DataFrame for the Wikipedia dataset with the estimated dates as columns
wiki_df_with_dates <- as.data.frame(
  cbind(
    Page=wiki_df$X1,
    as.data.frame(
      matrix(
        unlist(wiki_df[-1]),
        nrow=nrow(wiki_df),
        byrow=T)
      )
    )
  )

# Assign the generated dates as column names
colnames(wiki_df_with_dates)[-1] <- as.character(wiki_dates)

# View the resulting DataFrame
head(wiki_df_with_dates)

```

```{r remove non-wikipedia sites}
wikipedia_site_regex <- "[.]wikipedia[.]org"

filtered_wiki_df_with_dates <- wiki_df_with_dates %>% 
  filter(str_detect(Page, wikipedia_site_regex))
```

```{r add country/device}
pages <- filtered_wiki_df_with_dates$Page

names <- get_name(pages)

languages <- get_language(pages)

devices <- get_device(pages)

wiki_df_with_dates_country_device <- filtered_wiki_df_with_dates %>% 
  mutate(name = names, language = languages, device = devices) %>%
  relocate(c(name, language, device), .after=Page)
```

```{r remove spider pages}
filtered_wiki_df_with_dates_country_device <- wiki_df_with_dates_country_device %>% 
  filter(device != "all-access_spider")
head(filtered_wiki_df_with_dates_country_device)

```


```{r}
# Melting the data to have it in a long format suitable for time series analysis
data_long <- wiki_df_with_dates_country_device %>% 
  pivot_longer(cols = starts_with("2017")|starts_with("2018")|starts_with("2019")|starts_with("2020"),
               names_to = "date", values_to = "page_visits") %>% 
  mutate(date = as.Date(date, format = "%Y-%m-%d"))
  
```

```{r Base Model, Random Walk}

# Preparing data to build base model
base_data <- data_long %>%
  #mutate(Year = as.numeric(format(date,'%Y')), Month = as.numeric(format(date,'%m '))) %>%
  arrange(date) %>%
  arrange(name)

# Select out the name variable
name_order <- filtered_wiki_df_with_dates_country_device %>%
  arrange(name) %>%
  select(name)

# Transform the character variables into factors 
base_data$language <- as.factor(base_data$language)
base_data$device <- as.factor(base_data$device)

head(base_data)

# Encode categorical variables
dmy <- dummyVars(" ~ .", data = base_data[,3:4])
encoded_data <- data.frame(predict(dmy, newdata = base_data[,3:4]))
remaining_col <- base_data %>%
  select(- c("Page", "name","language","device"))

base_data <- cbind(encoded_data, remaining_col)

# Split training and testing dataset
base_train <- base_data %>%
  filter(date != "2020-01-16")
base_test <- base_data %>%
  filter(date == "2020-01-16")

xgb_model <- xgboost(data = model.matrix(~. - page_visits, data = base_train)[, -12],
                   label = base_train$page_visits,
                   max.depth = 50,
                   eta = 0.05,
                   nrounds = 100,
                   verbose = 2)

# Predict the model
xgb_pred <- predict(xgb_model, model.matrix(~. - page_visits, data = base_test)[, -12])

# View the results
xgb_results <- data.frame(Page = name_order,
                          Actual = base_test$page_visits, 
                          Prediction = xgb_pred)
head(xgb_results)

# View the Results of the base model 
data.frame(RMSE = rmse(xgb_results$Prediction, base_test$page_visits),
           MSE = mse(xgb_results$Prediction, base_test$page_visits),
           MAE = mae(xgb_results$Prediction, base_test$page_visits),
           MAPE = mape(xgb_results$Prediction, base_test$page_visits))

```









