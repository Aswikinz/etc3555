---
title: "wikipida"
author: "Aswin Kithalawaarachchi"
date: "2023-09-20"
output: html_document
---

```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(tidyverse)
library(urca)
library(fpp3)
library(naniar)
library(future)
library(furrr)

```


```{r get the country/device}
get_country <- function (name) {
  name_and_country_codes <- str_split(name, "[.]wikipedia[.]org") %>%
    map(first)
  country_codes <- str_split(name_and_country_codes, "_") %>%
    map(last)
  
  return (country_codes)
}

get_device <- function (name) {
  str_split(name, "[.]wikipedia[.]org_") %>%
    map(last)
}

get_name <- function (name) {
  name_and_country_codes <- str_split(name, "[.]wikipedia[.]org") %>%
    map(first)
  name <- gsub('.{3}$', '', name_and_country_codes)
  
  return (name)
  
}
```

```{r}
# Load the datasets
wiki_df <- read_csv('wikipedia.csv', col_names=FALSE)
train_df <- read_csv('filtered_train_2.csv')

# Extract the first time series from both datasets
wiki_series <- as.numeric(wiki_df[2, -1])
train_series <- as.numeric(train_df[2, -1])

# Normalize the series
wiki_series <- scale(wiki_series)
train_series <- scale(train_series)

# Initialize variables to store maximum correlation and its corresponding lag
max_corr <- -1
max_lag <- 0

# Loop over possible lags and calculate correlation for each lag
for (lag in 0:(min(length(wiki_series), length(train_series)) - 1)) {
  wiki_subseries <- wiki_series[(lag + 1):(min(length(wiki_series), length(train_series) + lag))]
  train_subseries <- train_series[1:(min(length(wiki_series) - lag, length(train_series)))]
  
  # Ensure the lengths of the series are equal before calculating the correlation
  if(length(wiki_subseries) == length(train_subseries)) {
    # Calculate correlation for the current lag
    current_corr <- cor(wiki_subseries, train_subseries)
    
    # Update maximum correlation and its corresponding lag if the current correlation is greater
    if (current_corr > max_corr) {
      max_corr <- current_corr
      max_lag <- lag
    }
  }
}

# Calculate the start date for the Wikipedia series based on the optimal lag
start_date <- as.Date(names(train_df)[2]) + max_lag

# Generate the date range for the Wikipedia series
wiki_dates <- seq(start_date, by='day', length.out=ncol(wiki_df) - 1)  # Correcting the length of the date sequence

# Create a new DataFrame for the Wikipedia dataset with the estimated dates as columns
wiki_df_with_dates <- as.data.frame(
  cbind(
    Page=wiki_df$X1,
    as.data.frame(
      matrix(
        unlist(wiki_df[-1]),
        nrow=nrow(wiki_df),
        byrow=T)
      )
    )
  )

# Assign the generated dates as column names
colnames(wiki_df_with_dates)[-1] <- as.character(wiki_dates)

# View the resulting DataFrame
head(wiki_df_with_dates)

```

```{r remove non-wikipedia sites}
wikipedia_site_regex <- "[.]wikipedia[.]org"

filtered_wiki_df_with_dates <- wiki_df_with_dates %>% 
  filter(str_detect(Page, wikipedia_site_regex))
```

```{r add country/device}
pages <- filtered_wiki_df_with_dates$Page

names <- get_name(pages)

countries <- get_country(pages)

devices <- get_device(pages)

wiki_df_with_dates_country_device <- filtered_wiki_df_with_dates %>% 
  mutate(name = names, country = countries, device = devices) %>%
  relocate(c(name, country, device), .after=Page)
```

```{r remove spider pages}
filtered_wiki_df_with_dates_country_device <- wiki_df_with_dates_country_device %>% 
  filter(device != "all-access_spider")
```

```{r pivot the data fro arima}
dataset_long <- filtered_wiki_df_with_dates %>%
  pivot_longer(cols = starts_with('2017'), names_to = 'Date', values_to = 'Web_Traffic') %>%
  mutate(Date = as.Date(Date, format = '%Y-%m-%d'))
```


```{r}
df_long <- filtered_wiki_df_with_dates_country_device %>% pivot_longer(
  cols = -c(Page, name, country, device),
  names_to = "date",
  values_to = "clicks"
)

n_dates <- nrow(filtered_wiki_df_with_dates_country_device) - 4
training_split_index <- ceiling(0.7 * n_dates)
  
training_split <- df_long %>% group_by(name, country, device) %>% 
  slice(1:training_split_index)

testing_split <- df_long %>% group_by(name, country, device) %>% 
  slice(training_split_index + 1: n_dates)
```

```{r new split training and testing dataset}
# Discuss which code chunk to use for training and testing data
# Define the split training row index
n_dates <- ncol(filtered_wiki_df_with_dates_country_device) - 4
training_split_index <- ceiling(0.7 * n_dates)

df_train <- filtered_wiki_df_with_dates_country_device %>%
  dplyr::select(1:training_split_index) %>% # select columns including the first 606 dates 
  pivot_longer(
  cols = -c(Page, name, country, device),
  names_to = "date",
  values_to = "clicks"
)


df_test <- filtered_wiki_df_with_dates_country_device %>%
  dplyr::select(-5:-training_split_index) %>% 
  pivot_longer(
  cols = -c(Page, name, country, device),
  names_to = "date",
  values_to = "clicks"
)

head(df_test)
  
```


```{r ets using df_long , eval = FALSE}
#u dont have to run this chunk, its already been processed and saved, skip to the next 

plan(multisession)

# Ensure 'models/' and 'forecasts/' directories exist
dir.create("models", showWarnings = FALSE)
dir.create("forecasts", showWarnings = FALSE)

df_long_filter <- df_long %>%
  select(Page, date, clicks) %>%
  mutate(date = as.Date(date, format="%Y-%m-%d"))

# Convert the data into a tsibble object (time series tibble)
data_tsibble <- df_long_filter %>% 
  as_tsibble(index = date, key = Page)

pages <- unique(df_long$Page)

page_filenames <- make.names(pages, unique = TRUE)

# Fit models in parallel and save them to RDS files
future_map2(pages, page_filenames, function(page, filename) {
  # Filter data for the specific page
  page_data <- data_tsibble %>% filter(Page == page)
  
  # Fit the ETS model
  model <- page_data %>%
    model(ETS(clicks))
  
  # Save the model to an RDS file, named by page
  saveRDS(model, paste0("models/", filename, ".rds"))
})

# Generate forecasts in parallel and save them
future_map(page_filenames, function(filename) {
  # Load the model from the RDS file
  model <- readRDS(paste0("models/", filename, ".rds"))
  
  # Generate forecasts
  forecasts <- model %>% forecast(h = "30 days")
  
  # Save the forecasts to an RDS file
  saveRDS(forecasts, paste0("forecasts/", filename, ".rds"))
})

# When needed, load forecasts and combine them for plotting/analysis
forecasts_list <- future_map(page_filenames, ~readRDS(paste0("forecasts/", .x, ".rds")))
forecasts_combined <- bind_rows(forecasts_list)


```


```{r load the dataframe back from the files}
# File paths
forcast_files <- list.files(path = "forecasts/", pattern = "*.rds", full.names = TRUE)
model_files <- list.files(path = "models/", pattern = "*.rds", full.names = TRUE)

# Read and bind data
forcast_list <- lapply(forcast_files, readRDS)
model_list <- lapply(model_files, readRDS)

forcast_ets_df <- bind_rows(forcast_list)
model_ets_df <- bind_rows(model_list)
```






```{r}
training_wide <- training_split %>% pivot_wider(names_from = date, values_from = clicks)

testing_wide <- testing_split %>% pivot_wider(names_from = date, values_from = clicks)
```




```{r}

# Scale your data (replace with your scaling logic)
scaled_train_data <- scale(train_data)
scaled_test_data <- scale(test_data)

# Create sequences (adjust window_size accordingly)
window_size <- 10
X_train <- array(0, dim = c(nrow(scaled_train_data) - window_size + 1, window_size, ncol(scaled_train_data)))
Y_train <- matrix(0, nrow = nrow(scaled_train_data) - window_size + 1, ncol = ncol(scaled_train_data))
for (i in 1:(nrow(scaled_train_data) - window_size + 1)) {
  X_train[i,,] <- scaled_train_data[i:(i + window_size - 1), ]
  Y_train[i,] <- scaled_train_data[i + window_size, ]
}
X_test <- array(0, dim = c(nrow(scaled_test_data) - window_size + 1, window_size, ncol(scaled_test_data)))
Y_test <- matrix(0, nrow = nrow(scaled_test_data) - window_size + 1, ncol = ncol(scaled_test_data))

for (i in 1:(nrow(scaled_test_data) - window_size + 1)) {
  X_test[i,,] <- scaled_test_data[i:(i + window_size - 1), ]
  Y_test[i,] <- scaled_test_data[i + window_size, ]
}

```




```{r neuron network model 1}

# Intialise the list for comparision between 2 models
model_list <- list()

model_list[["Model 1"]]$model <- keras_model_sequential() %>%
  layer_dense(units = 500, activation = 'relu', input_shape = c(520,3))%>% # input_shape = dates in X_train
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 390, activation = 'relu') %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 256, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 128, activation = 'tanh')
```
  
```{r}
model_list[["Model 1"]]$model_compiled <-  model_list[["Model 1"]]$model %>%
  compile(loss = "mean_squared_error",
    optimizer = 'adam',
    metrics = c("accuracy"))
```


```{r train the model}

model_list[["Model 1"]]$fit <- model_list[["Model 1"]]$model %>%
  fit(df_train,
      df_test,
      epoch = 2, # number of iteration
      batch_size = 128,
      validation_split = 0.2)

```


```{r evaluation}
evaluate_df <- function(model){
  evaluate(model, X_test, Y_test, verbose = 0) %>%
    bind_rows()
}

# Convert list to data frame 
model_df <- data_frame(
  name = fct_inorder(names(model_list)),
  model = map(model_list, "model"),
  fit = map(model_list, "fit")
) %>%
  mutate(eval = map(model, evaluate_df)) %>%
  unnest(eval)

# Loss and accuracy plot
model_df %>%
  mutate(metrics = map(fit, function(x){data.frame(x)})) %>%
  select(name, metrics) %>%
  unnest(metrics) %>%
  ggplot(aes(x = epoch, y = value, colour = data)) +
  geom_line() +
  facet_grid(metric~name, scales = "free_y")

# Loss and accuracy summary
model_df %>%
  select(name, loss, accuracy) %>%
  knitr::kable(caption = "Loss and Accuracy for 2 models")

# Compare loss and accuracy between models 
model_df %>%
  select(name, loss, accuracy) %>%
  gather(var,val, -name) %>%
  ggplot(aes(x = name, y = val)) +
  geom_col(colour = "blue", fill = "blue", alpha = 0.2, width = 0) +
  geom_point(colour = "blue") +
  facet_wrap(~var, scales = "free_x") +
  coord_flip()

```










